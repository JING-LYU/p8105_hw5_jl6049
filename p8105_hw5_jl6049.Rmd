---
title: "p8105_hw5_jl6049"
author: "LYU JING"
date: "11/13/2021"
output: html_document
---


```{r message=FALSE}
library(tidyverse)
```


```{r}
homicide = read.csv("homicide-data.csv")
unique(homicide$disposition)
```
The dataset contains *`r count(homicide)`* observations of homicides in 50 large U.S. cities.

The dataset has 12 columns, including:

**numeric variables:**

* `lat`
* `lon`
* `reported_date`


**character variables:**

* `uid`
* `victim_last`
* `victim_first`
* `victim_race`
* `victim_age`
* `victim_sex`
* `city`
* `state`
* `disposition`

From the dataset, we could know the basic information of the victim in a homicide(name, race, age and sex), place of the homicide and disposition of the homicide.

```{r}
homicide_number =
  homicide %>% 
  mutate(city_state = paste(city,state,sep = ",")) %>% 
  group_by(city) %>% 
  summarise(n_homicide = n(),
            n_unsolved = sum((disposition == "Closed without arrest") | (disposition == "Open/No arrest"))
            )

n_baltimore_homicide = homicide_number %>% 
  filter(city == "Baltimore") %>% 
  pull(n_homicide)

n_baltimore_usolved = homicide_number %>% 
  filter(city == "Baltimore") %>% 
  pull(n_unsolved)

test = prop.test(n_baltimore_usolved,n_baltimore_homicide)
tidy_test = broom::tidy(test)

estimated_proportion = round(pull(tidy_test,estimate),2)
confidence_low  = round(pull(tidy_test,conf.low),2)
confidence_high  = round(pull(tidy_test,conf.high),2)

tibble(estimated_proportion,confidence_low,confidence_high)
```
Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}




est_prop_ci = function(x){
  
  test = 
    prop.test(x$n_unsolved, x$n_homicide) %>% 
    broom::tidy()
  
  estimated_proportion = round(pull(test,estimate),2)
  confidence_low  = round(pull(test,conf.low),2)
  confidence_high  = round(pull(test,conf.high),2)

tibble(estimated_proportion,confidence_low,confidence_high)
}

homicide_number_test_result =
  homicide_number %>% 
  nest(data = n_homicide:n_unsolved) %>% 
  mutate(summary = map(data, est_prop_ci)) %>% 
  select(-data) %>% 
  unnest(summary)

knitr::kable(homicide_number_test_result)


```

```{r}
homicide_number_test_result %>% 
  mutate(
    city = fct_reorder(city,estimated_proportion)
    ) %>%
  ggplot() + 
  geom_errorbar(aes(x=city, ymin = confidence_low, ymax = confidence_high), width = 0.2, size=1, color = "blue") + 
  geom_point(aes(x = city,y = estimated_proportion),size = 1,shape = 21,fill = "white") +
  theme(text = element_text(size=9)) +
  coord_flip() +
  labs(
    title = "Estimates proportion and CIs of the number of unsolved homicides for each city",
    x = "City",
    y = "Value"
  )
```

# Problem 2

```{r}
list_file = list.files(path = "data")

read_data = function(x){
  data = read.csv(paste("data/",x, sep = "")) %>% 
    mutate(name = x)

}

data_participant = 
  map(list_file, read_data) %>% 
  bind_rows() %>% 
  relocate(name) %>% 
  mutate(name = str_remove(name, '.csv')) %>% 
  separate(name, into = c("arm", "subject_ID"), sep = "_") %>% 
  filter(arm =="con") %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week", 
    values_to = "data") %>% 
  mutate(
    week = str_replace(week, "_", " ")
  )
  

  
```

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
data_participant %>% 
  ggplot(aes(x = week, y = data, group = subject_ID)) + 
  geom_line(aes(color = subject_ID)) + 
  theme(legend.position = "right") + 
  labs(
    title = "Spaghetti plot - 8 weeks data of participants in control arm",
    subtitle = "week1 - week8",
  )
```
The differences beween the data of 10 participants in control arm have patterns as follows:

1. difference in week 5 is the largest and then gradually converge in next several weeks. 

2. the difference in week 8 is the smallest.

3. seems like the difference are from chaos to order


# problem 3

```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```


```{r}
fill_missing = function(x){
  if (is.numeric(x)) {

    replace_na(x,round(mean(x,na.rm = TRUE),1))
  } else if (is.character(x)) {
    replace_na(x,"virginica")
  }else{
    "wrong"
  }
}

iris_recover = 
  iris_with_missing %>% 
  map_df(fill_missing)
  

iris_recover
```

